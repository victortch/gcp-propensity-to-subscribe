"""Persist training metrics to BigQuery (behavior-neutral).

Two tables are managed:

``train_metrics``
    Compact per-label summary (mean validation AUC, chosen threshold, params).

``train_metrics_detail``
    Wide table mirroring the CSV generated by :mod:`app.training.train`,
    containing per-fold validation metrics, expected metrics, and baselines.

The code here is storage-only and does not affect model behavior.
"""

from __future__ import annotations

from datetime import datetime
from typing import Any, Dict, List

import json
import pandas as pd
from google.cloud import bigquery

from app.common.io import get_bq_client
from app.common.bq_utils import create_table_if_not_exists


# BigQuery schema for propensity_to_subscribe.train_metrics
_SUMMARY_SCHEMA: List[bigquery.SchemaField] = [
    bigquery.SchemaField("run_id", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("label_tag", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("auc_val_mean", "FLOAT", mode="REQUIRED"),
    bigquery.SchemaField("threshold_expected", "FLOAT", mode="REQUIRED"),
    bigquery.SchemaField("params_json", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("created_at", "TIMESTAMP", mode="REQUIRED"),
]


# Wide schema for propensity_to_subscribe.train_metrics_detail
_DETAIL_SCHEMA: List[bigquery.SchemaField] = [
    bigquery.SchemaField("run_id", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("label", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("split", "STRING", mode="REQUIRED"),
    bigquery.SchemaField("fold", "INT64", mode="REQUIRED"),
    bigquery.SchemaField("n", "INT64", mode="NULLABLE"),
    bigquery.SchemaField("pos", "INT64", mode="NULLABLE"),
    bigquery.SchemaField("neg", "INT64", mode="NULLABLE"),
    bigquery.SchemaField("auc", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("pr_auc", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("logloss", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("brier", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("ks", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("precision", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("recall", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("accuracy", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("f1", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("threshold", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("tp", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("fp", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("tn", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("fn", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_pos", "INT64", mode="NULLABLE"),
    bigquery.SchemaField("exp_neg", "INT64", mode="NULLABLE"),
    bigquery.SchemaField("exp_pr_auc", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_logloss", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_brier", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_tp", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_fp", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_tn", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_fn", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_precision", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_recall", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_accuracy", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("exp_f1", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("tpr", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("fpr", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_prob_obs_auc", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_prob_obs_pr_auc", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_prob_obs_logloss", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_prob_obs_brier", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_prob_exp_auc", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_prob_exp_pr_auc", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_prob_exp_logloss", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_prob_exp_brier", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_obs_tp", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_obs_fp", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_obs_tn", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_obs_fn", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_obs_precision", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_obs_recall", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_obs_accuracy", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_obs_f1", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_exp_tp", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_exp_fp", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_exp_tn", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_exp_fn", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_exp_precision", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_exp_recall", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_exp_accuracy", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rand_exp_f1", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_rate_pred", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("base_p_const", "FLOAT", mode="NULLABLE"),
    bigquery.SchemaField("ingested_at", "TIMESTAMP", mode="REQUIRED"),
]


def _ensure_table(project_id: str, dataset: str, table: str) -> str:
    """
    Create the metrics table if missing (idempotent) and return its FQN.
    """
    table_fqn = f"{project_id}.{dataset}.{table}"
    create_table_if_not_exists(
        project=project_id,
        dataset_id=dataset,
        table_id=table,
        schema=_SUMMARY_SCHEMA,
        partition_field=None,          # simple append-only table
        clustering_fields=None,
        description="Economedia PTS training metrics (one row per label and run).",
        labels={"purpose": "pts-train-metrics"},
    )
    return table_fqn


def _ensure_detail_table(project_id: str, dataset: str, table: str) -> str:
    table_fqn = f"{project_id}.{dataset}.{table}_detail"
    create_table_if_not_exists(
        project=project_id,
        dataset_id=dataset,
        table_id=f"{table}_detail",
        schema=_DETAIL_SCHEMA,
        partition_field=None,
        clustering_fields=["label", "split"],
        description=(
            "Economedia PTS training metrics detail (per fold/test, observed & expected "
            "metrics plus baselines)."
        ),
        labels={"purpose": "pts-train-metrics-detail"},
    )
    return table_fqn


def write_training_summary(
    *,
    project_id: str,
    dataset: str,
    table: str,
    run_id: str,
    label_tag: str,
    auc_val_mean: float,
    threshold_expected: float,
    params: Dict[str, Any],
) -> None:
    """
    Append a single summary row to the train_metrics table.

    Args:
        project_id, dataset, table: BigQuery destination identifiers.
        run_id: training run identifier.
        label_tag: e.g., "cap_30d".
        auc_val_mean: mean validation ROC AUC across folds.
        threshold_expected: chosen global threshold (expected F1).
        params: best hyperparameters dict (stored as JSON string).
    """
    table_fqn = _ensure_table(project_id, dataset, table)
    client = get_bq_client(project_id)

    row = {
        "run_id": run_id,
        "label_tag": label_tag,
        "auc_val_mean": float(auc_val_mean),
        "threshold_expected": float(threshold_expected),
        "params_json": json.dumps(params, ensure_ascii=False, sort_keys=True),
        "created_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
    }
    df = pd.DataFrame([row])

    job = client.load_table_from_dataframe(
        df,
        table_fqn,
        job_config=bigquery.LoadJobConfig(
            write_disposition="WRITE_APPEND",
        ),
    )
    job.result()


def write_metrics_detail(
    *,
    project_id: str,
    dataset: str,
    table: str,
    metrics_df: pd.DataFrame,
) -> None:
    """Load the detailed metrics DataFrame (per-fold/test) into BigQuery."""

    if metrics_df.empty:
        return

    table_fqn = _ensure_detail_table(project_id, dataset, table)
    client = get_bq_client(project_id)

    df = metrics_df.copy()
    df["ingested_at"] = datetime.utcnow().isoformat(timespec="seconds") + "Z"

    job = client.load_table_from_dataframe(
        df,
        table_fqn,
        job_config=bigquery.LoadJobConfig(write_disposition="WRITE_APPEND"),
    )
    job.result()